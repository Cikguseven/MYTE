{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import ByT5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from src.myt5_tokenizer import MyT5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sentence Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\"የአውሮፕላን አብራሪው የአየር ሀይል መሪ ዲሎክሪት ፓታቪ ሆኖ ተለይቷል።\",\n",
    "             \"The pilot was identified as Squadron Leader Dilokrit Pattavee.\",\n",
    "             \"Der Pilot wurde als Staffelführer Dilokrit Pattavee identifiziert.\",\n",
    "             \"Išsiaiškinta, kad pilotas – eskadrilės vadas Dilokritas Pattavee.\",\n",
    "             \"涉事飞行员是空军中队长迪罗里·帕塔维 (Dilokrit Pattavee)。\",\n",
    "             \"Pilota zidentyfikowano jako Dilokrita Pattavee, dowódcę eskadry.\"]\n",
    "\n",
    "sentences_2 =  ['We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.',\n",
    "                \"አሁን የስኳር በሽተኛ ያልነበሩ አሁን ግን የሆኑ የ4-ወር-ዕድሜ ያላቸው አይጦች አሉን፣ አለ። \",\n",
    "                \"„Mamy teraz myszy w wieku 4 miesięcy, które miały cukrzycę, ale zostały z niej wyleczone” – dodał.\",\n",
    "                \"「我們有 4 個月大曾經罹患糖尿病老鼠現在沒有糖尿病了」他補充道。\"]\n",
    "\n",
    "prefixes = [sent[:5] for sent in sentences]\n",
    "sufixes = [sent[5:] for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Byte and Morpholofical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "byt5_small = T5ForConditionalGeneration.from_pretrained(\"hf_checkpoints/byt5_small_250000\" ,use_safetensors=True)\n",
    "by_tokenizer = ByT5Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "myt5_small = T5ForConditionalGeneration.from_pretrained(\"hf_checkpoints/myt5_small_250000\" ,use_safetensors=True)\n",
    "my_tokenizer = MyT5Tokenizer(decompose_map=\"byte_maps/decompose_map.json\",\n",
    "                               merge_map=\"byte_maps/merge_map.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODOs\n",
    "- check how loss is computed\n",
    "- run on Flores\n",
    "- compare multiple models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate NLL on sentence levels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_texts(text_dataset, model, tokenizer, batch_size=32, context=0):\n",
    "\n",
    "    sentence_nlls = []\n",
    "    sentence_bpbs = []\n",
    "    sentence_compressions = []\n",
    "    context = min(abs(context), 1.0)\n",
    "\n",
    "    for i in tqdm(range(0, len(text_dataset), batch_size)):\n",
    "        batch = text_dataset[i:i+batch_size]\n",
    "        batch_contexts = [math.floor(context * len(sent.split(\" \"))) for sent in batch]\n",
    "\n",
    "        batch_prefixes = [\" \".join(sent.split(\" \")[:bc]) + \" \" for sent, bc in zip(batch,batch_contexts) ]\n",
    "        batch_suffixes = [\" \".join(sent.split(\" \")[bc:]) for sent, bc in zip(batch,batch_contexts) ]\n",
    "        byte_lengths = torch.tensor([len(suf.encode(\"utf-8\")) +1 for suf in batch_suffixes])\n",
    "        if len(batch_prefixes) == 0:\n",
    "            continue\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prefixes, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = tokenizer(\n",
    "            batch_suffixes, padding=\"longest\", return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        outputs = model(**inputs, labels=targets.input_ids)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        logits = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        target_labels = targets.input_ids.unsqueeze(-1)\n",
    "        mask = targets.attention_mask\n",
    "\n",
    "        target_logits = torch.gather(logits, -1, target_labels).squeeze(-1)\n",
    "\n",
    "        batch_nlls = -torch.sum(mask * target_logits, axis=-1)\n",
    "        batch_bpbs = torch.exp(-torch.sum(mask * target_logits , axis=-1)/byte_lengths)\n",
    "        batch_compressions = torch.sum(mask, axis=-1) / byte_lengths\n",
    "        print(outputs.loss * batch_compressions)\n",
    "        sentence_nlls.extend(batch_nlls.tolist())\n",
    "        sentence_bpbs.extend(batch_bpbs.tolist())\n",
    "        sentence_compressions.extend(batch_compressions.tolist())\n",
    "\n",
    "\n",
    "    return sentence_nlls, sentence_bpbs, sentence_compressions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def evaluate_texts(text_dataset, model, tokenizer, batch_size=32):\n",
    "\n",
    "#     sentence_nlls = []\n",
    "#     sentence_bpbs = []\n",
    "#     #context = min(abs(context), 1.0)\n",
    "\n",
    "#     for i in tqdm(range(0, len(text_dataset), batch_size)):\n",
    "#         batch = text_dataset[i:i+batch_size]\n",
    "#         # batch_contexts = [math.floor(context * len(sent)) for sent in batch]\n",
    "\n",
    "#         # batch_prefixes = [sent[:bc] for sent, bc in zip(batch,batch_contexts) ]\n",
    "#         # batch_suffixes = [sent[bc:] for sent, bc in zip(batch,batch_contexts) ]\n",
    "#         byte_lengths = torch.tensor([len(sent.encode(\"utf-8\")) for sent in batch])\n",
    "#         # if len(batch_prefixes) == 0:\n",
    "#         #     continue\n",
    "\n",
    "#         inputs = tokenizer(\n",
    "#             batch, padding=\"longest\", return_tensors=\"pt\", add_special_tokens = True\n",
    "#         )\n",
    "#         # targets = tokenizer(\n",
    "#         #     batch_suffixes, padding=\"longest\", return_tensors=\"pt\"\n",
    "#         # )\n",
    "    \n",
    "#         targets = inputs.input_ids.clone()\n",
    "\n",
    "#         outputs = model(**inputs, labels=targets)\n",
    "#         print(outputs.loss)\n",
    "        \n",
    "#         logits = outputs.logits[:, :-1, :]\n",
    "#         print(logits.shape)\n",
    "#         logits = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "#         #probabilities = \n",
    "#         target_labels = targets[:,1:].unsqueeze(-1)\n",
    "#         mask = inputs.attention_mask[:,:-1]\n",
    "\n",
    "#         target_logits = torch.gather(logits, -1, target_labels).squeeze(-1)\n",
    "\n",
    "#         #target_logits = torch.gather(logits, -1, target_labels).squeeze(-1)\n",
    "\n",
    "#         batch_nlls = -torch.sum(mask * target_logits, axis=-1)\n",
    "#         batch_bpbs = torch.exp(-torch.sum(mask * target_logits , axis=-1)/byte_lengths)\n",
    "\n",
    "#         sentence_nlls.extend(batch_nlls.tolist())\n",
    "#         sentence_bpbs.extend(batch_bpbs.tolist())\n",
    "\n",
    "#     return outputs, sentence_nlls, sentence_bpbs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = [\"የአውሮፕላን አብራሪው የአየር ሀይል መሪ ዲሎክሪት ፓታቪ ሆኖ ተለይቷል።\",\n",
    "             \"The pilot was identified as Squadron Leader Dilokrit Pattavee.\",\n",
    "             \"Der Pilot wurde als Staffelführer Dilokrit Pattavee identifiziert.\",\n",
    "             \"Išsiaiškinta, kad pilotas – eskadrilės vadas Dilokritas Pattavee.\",\n",
    "             \"涉事飞行员是空军中队长迪罗里·帕塔维 (Dilokrit Pattavee)。\",\n",
    "             \"Pilota zidentyfikowano jako Dilokrita Pattavee, dowódcę eskadry.\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts([sentences[0]], myt5_small, my_tokenizer, context=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts(sentences, myt5_small, my_tokenizer, context=0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts(sentences, byt5_small, by_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts(sentences_2, myt5_small, my_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts(sentences_2, byt5_small, by_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts([sentences_2[3]], byt5_small, by_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_texts(sentences, byt5_small, by_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Flores Dataset for some language"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load flores dataset for languages: en, es, pt, fr, it, ro, pl, mt, ja, zh, ko, te, ta\n",
    "\n",
    "# language sample\n",
    "languages = ['en', 'de', 'fr', 'ru','pl','ja', 'vi', 'ko','hy', 'kk', 'el', 'ta','te','am', 'sn', 'mt', 'sm', 'st']\n",
    "\n",
    "palette = sns.color_palette(\"viridis\", len(languages))\n",
    "languages_colors = {lang: col for lang, col in zip(languages, palette)}\n",
    "nice_colors = [ ]\n",
    "\n",
    "\n",
    "\n",
    "# use the code from above to get the flores200 languages\n",
    "# Latin / Corsican / Hawaiian language not in Flores\n",
    "languages_flores = {'en': 'eng_Latn', 'ceb': 'ceb_Latn', 'de': 'deu_Latn', 'sv': 'swe_Latn', 'fr': 'fra_Latn', 'nl': 'nld_Latn', 'ru': 'rus_Cyrl', 'es': 'spa_Latn',\n",
    "                    'it': 'ita_Latn', 'pl': 'pol_Latn', 'ja': 'jpn_Jpan', 'zh': 'zho_Hans', 'uk': 'ukr_Cyrl', 'vi': 'vie_Latn', 'ar': 'arb_Arab',\n",
    "                    'pt': 'por_Latn', 'fa': 'pes_Arab', 'ca': 'cat_Latn', 'sr': 'srp_Cyrl', 'id': 'ind_Latn', 'ko': 'kor_Hang', 'no': 'nob_Latn',\n",
    "                    'fi': 'fin_Latn', 'tr': 'tur_Latn', 'cs': 'ces_Latn', 'hu': 'hun_Latn', 'ro': 'ron_Latn', 'eu': 'eus_Latn', 'ms': 'zsm_Latn',\n",
    "                    'eo': 'epo_Latn', 'he': 'heb_Hebr', 'hy': 'hye_Armn', 'da': 'dan_Latn', 'bg': 'bul_Cyrl', 'cy': 'cym_Latn', 'sk': 'slk_Latn',\n",
    "                    'uz': 'uzn_Latn', 'et': 'est_Latn', 'be': 'bel_Cyrl', 'kk': 'kaz_Cyrl', 'el': 'ell_Grek', 'lt': 'lit_Latn', 'gl': 'glg_Latn',\n",
    "                    'ur': 'urd_Arab', 'az': 'azj_Latn', 'sl': 'slv_Latn', 'ka': 'kat_Geor', 'hi': 'hin_Deva', 'th': 'tha_Thai', 'ta': 'tam_Taml',\n",
    "                    'bn': 'ben_Beng', 'mk': 'mkd_Cyrl',  'lv': 'lvs_Latn', 'af': 'afr_Latn', 'tg': 'tgk_Cyrl', 'my': 'mya_Mymr',\n",
    "                    'mg': 'plt_Latn', 'sq': 'als_Latn', 'mr': 'mar_Deva', 'te': 'tel_Telu', 'ml': 'mal_Mlym', 'ky': 'kir_Cyrl', 'sw': 'swh_Latn',\n",
    "                    'jv': 'jav_Latn', 'ht': 'hat_Latn', 'lb': 'ltz_Latn', 'su': 'sun_Latn', 'ku': 'kmr_Latn', 'ga': 'gle_Latn', 'is': 'isl_Latn',\n",
    "                    'fy': 'fao_Latn', 'pa': 'pan_Guru', 'yo': 'yor_Latn', 'ne': 'npi_Deva', 'ha': 'hau_Latn', 'kn': 'kan_Knda', 'gu': 'guj_Gujr',\n",
    "                    'mn': 'khk_Cyrl', 'ig': 'ibo_Latn', 'si': 'sin_Sinh', 'ps': 'pbt_Arab', 'gd': 'gla_Latn', 'sd': 'snd_Arab', 'yi': 'ydd_Hebr',\n",
    "                    'am': 'amh_Ethi', 'sn': 'sna_Latn', 'zu': 'zul_Latn', 'km': 'khm_Khmr', 'so': 'som_Latn', 'mi': 'mri_Latn',\n",
    "                    'mt': 'mlt_Latn', 'lo': 'lao_Laoo', 'xh': 'xho_Latn', 'sm': 'smo_Latn', 'ny': 'nya_Latn', 'st': 'sot_Latn'}\n",
    "\n",
    "flores = {}\n",
    "\n",
    "for lang in languages:\n",
    "    with open(f'flores200_dataset/devtest/{languages_flores[lang]}.devtest', 'r') as f:\n",
    "        flores[lang] = f.read().splitlines()[:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpbs = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}\n",
    "nlls = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}\n",
    "comps = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpbs_75 = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}\n",
    "nlls_75 = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}\n",
    "comps_75 = {\"my_small\": {}, \"by_small\": {}, \"mixed_small\": {}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "model = \"my_small\"\n",
    "\n",
    "bpbs_75[model][lang], nlls_75[model][lang], comps_75[model][lang] = evaluate_texts(flores[lang], myt5_small, my_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "model = \"by_small\"\n",
    "\n",
    "bpbs_75[model][lang], nlls_75[model][lang], comps_75[model][lang] = evaluate_texts(flores[lang], byt5_small, by_tokenizer, context=0.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.median(bpbs[\"my_small\"][\"en\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(nlls_75[\"my_small\"][\"en\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(nlls_75[\"by_small\"][\"en\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}